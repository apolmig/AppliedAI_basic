{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AppliedAI_6_Pelis_semantic-similarity-chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apolmig/AppliedAI_basic/blob/master/AppliedAI_6_Pelis_semantic_similarity_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R0T0ei52FXS",
        "colab_type": "text"
      },
      "source": [
        "# Semantic similarity chatbot (with movie dialog)\n",
        "\n",
        "By [Allison Parrish](http://www.decontextualize.com/)\n",
        "\n",
        "![bot screenshot](http://static.decontextualize.com/snaps/semantic-similarity-chatbot.png)\n",
        "\n",
        "I teach [programming, arts and design](https://itp.nyu.edu/) and a perennial project idea is to make a chatbot that mimics someone or something—a famous author, a historical figure, or even the student's own e-mails or messaging logs. This notebook and the software described herein is intended to give those students some sample code to work with and a bit of a head start on concepts and architecture. (In particular, this material was inspired by conversations I had with [Utsav Chadha](https://itp.nyu.edu/thesis2018/#/student/utsav-chadha) and [Nouf Aljowaysir](https://itp.nyu.edu/thesis2018/#/student/nouf-aljowaysir) during the Spring 2018 semester at ITP.)\n",
        "\n",
        "In the notebook, I'll show how the chatbot works and build an example chatbot using the [Cornell Movie Dialog Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). Even if you don't know anything about programming or natural language processing or machine learning or whatever, you can step through the cells in this notebook and play around with the chatbot itself at the very end.\n",
        "\n",
        "> **TLDR version**: To run the chatbot, just keep hitting shift+enter until you reach the end. (A bunch of stuff needs to download and build, so it'll take a few minutes. Sorry.) If you're using Google Colab, there will be a little chat widget right in the notebook. If you're in Jupyter Notebook, there will be a link you can click on to open the chat in a new browser window.\n",
        "\n",
        "> **Content warning**: The Cornell Movie Dialog Corpus has dialog from many movies, including some with potentially objectionable content. When playing around with this code, you might see text from the dialog of these films, including (in some cases) violent language and slurs directed at marginalized groups. If you make a chatbot with this code and this corpus and make it available to a wide audience, consider including a content warning similar to this one and/or filtering the corpus and output of the bot to exclude words and sentiments like this.\n",
        "\n",
        "## Making a chatbot the easy way\n",
        "\n",
        "There are [lots](https://www.rivescript.com/) [of](https://rasa.com/) [ways](https://botpress.io/) to author chatbots, but many of them are oriented toward particular use cases (i.e., automating customer service), and require extensive hand-authoring of content or hand-labelling of data. Others (i.e., those that use seq2seq) require you to train a neural network from scratch, which is fine if you're into that kind of thing, but can sometimes feel like a rotten way to spend your money and your afternoon (or weekend, or month, or whatever).\n",
        "\n",
        "The chatbot in this notebook won't pass a Turing test or push percentage points on any machine learning accuracy evaluations, but it's (a) easy to understand (b) works with any corpus (c) doesn't require training a new model and (d) uncannily faithful to whatever source material you give it while still being amusingly bizarre. From a technical perspective, you can think of it as a sort of low-rent version of [Talk to Books](https://books.google.com/talktobooks/), which (as I understand it) works along similar principles.\n",
        "\n",
        "So how does this chatbot work? To answer that question we have to think about how *conversations* work.\n",
        "\n",
        "### Defining the conversation\n",
        "\n",
        "For the purposes of this chatbot, let's make a very simple \"toy\" definition of conversation. We'll say that a conversation consists of *two people taking turns at making utterances.* We'll call any individual utterance a *turn*. When one participant finishes their turn, the next participant can take their own turn; we'll call this second turn a *response* to the first. The conversation continues this way, with each turn being a response to the previous turn, until it comes to an end (usually due to a mutual agreement reached by the participants, which in the case of our chatbot, means whenever the human gets sick of chatting and closes the browser tab).\n",
        "\n",
        "To illustrate, here's a simple conversation I just invented between two participants, A and B. The first column numbers the turns, the second column labels the participant, and the third column gives the text of the turn:\n",
        "\n",
        "| # | P | Text |\n",
        "|-|-|:-|\n",
        "| 1 | A | Hello. |\n",
        "| 2 | B | Good to see you! |\n",
        "| 3 | A | I'm reading a tutorial on semantic similarity and chatbots. It's quite interesting. |\n",
        "| 4 | B | Thanks for letting me know. |\n",
        "| 5 | A | Any time. Well, I gotta go. |\n",
        "| 6 | B | Talk to you soon! |\n",
        "| 7 | A | Goodbye. |\n",
        "\n",
        "This fascinating conversation has seven turns. Turn 2 is the response to turn 1, turn 3 is the response to turn 2, etc.\n",
        "\n",
        "> *Note:* I said this was a \"toy\" definition for a reason—conversations are actually *way* more complicated than this. If you're interested in how conversations actually work, check out [conversation analysis](https://en.wikipedia.org/wiki/Conversation_analysis), a whole subfield of linguistics devoted to this kind of thing.\n",
        "\n",
        "### Taking a turn\n",
        "\n",
        "At a certain basic level, the job of a chatbot at any moment in a conversation is to produce a conversational turn that seems to plausibly be in response to the turn that preceded it. There are a number of different ways to solve this problem. Our strategy is going to be the following:\n",
        "\n",
        "1. Make a database of conversations and the turns that constitute them;\n",
        "2. Assign a *vector* to each turn that corresponds to its meaning (more on this in a second);\n",
        "3. When asked to respond to a conversational turn from the user, display the *response* to the turn in the database most similar in meaning to the user's turn.\n",
        "\n",
        "For example, take the conversation that I invented earlier. Imagine putting all of these turns into the database and assigning each turn a vector representing its meaning. Our chatbot now has a database of six possible responses (not counting the first turn, since it began the conversation and wasn't in response to any other turn). If the user typed in something like...\n",
        "\n",
        "    > Howdy!\n",
        "    \n",
        "... our chatbot would then search its database for the turn closest in meaning to `Howdy!` Maybe that turn is turn #1 (`Hello.`). The chatbot would then display the turn that happened *in response* to turn #1 (i.e., turn #2, `Good to see you!`). If the user typed in...\n",
        "\n",
        "    > Thank you for the great conversation!\n",
        "    \n",
        "... our chatbot would find the turn in its database closest in meaning, maybe turn #4 (`Thanks for letting me know.`), and then print out its associated response (turn #5, `Any time. Well, I gotta go.`). The final transcript of this imaginary (and admittedly a little contrived) conversation, with the human's turn labelled with `H` and the bot as `B`:\n",
        "\n",
        "    H: Howdy!\n",
        "    B: Hello.\n",
        "    H: Thank you for the great conversation!\n",
        "    B: Any time. Well, I gotta go!\n",
        "    \n",
        "Perfectly plausible!\n",
        "\n",
        "So you can think of this semantic similarity chatbot as a kind of search engine. When you type something into the chat, the chatbot *searches its database for the most appropriate response*.\n",
        "\n",
        "### Word vectors\n",
        "\n",
        "\"This is all well and good,\" you say. \"But how do you make a computer program that knows how similar in meaning two sentences are? How do you even *measure* similarity in meaning?\" Figuring out a way to measure similarity in meaning is one of the classic problems in computational linguistics, and it's still very much an open problem. But there are certain easy-to-use techniques that are \"good enough\" for our purposes. In particular, we're going to use *word vectors*.\n",
        "\n",
        "[I've written a more detailed introduction to word vectors here](https://github.com/aparrish/rwet/blob/master/understanding-word-vectors.ipynb), if you want the whole story. But the short version is this: using machine learning techniques and a lot of data, it's possible to assign each word a sequence of numbers (i.e., a vector) that encodes the word's meaning. (Actually, it's encoding the word's *distribution*, or all of the other words that the word is usually seen alongside. But it turns out that this is a good substitute for representing a word's meaning.) \n",
        "\n",
        "A word vector looks a lot like the Cartesian X, Y coordinates you likely studied in school, except that they usually have many hundreds of dimensions, not just two. (More dimensions means more information about the word's distribution.) For example, here's the vector for the word \"cheese\" using the fifty-dimensional pre-trained vectors from GloVe:\n",
        "\n",
        "    -0.053903 -0.30871 -1.3285 -0.43342 0.31779 1.5224 -0.6965 -0.037086 -0.83784 0.074107 -0.30532 -0.1783 1.2337 0.085473 0.17362 -0.19001 0.36907 0.49454 -0.024311 -1.0535 0.5237 -1.1489 0.95093 1.1538 -0.52286 -0.14931 -0.97614 1.3912 0.79875 -0.72134 1.5411 -0.15928 -0.30472 1.7265 0.13124 -0.054023 -0.74212 1.675 1.9502 -0.53274 1.1359 0.20027 0.02245 -0.39379 1.0609 1.585 0.17889 0.43556 0.68161 0.066202\n",
        "\n",
        "Experts have made [large databases of word vectors available for people to download and use](https://nlp.stanford.edu/projects/glove/), so that you don't have to train them yourself. (Though [you can train them yourself if you want to](https://radimrehurek.com/gensim/models/word2vec.html).)\n",
        "\n",
        "### Sentence vectors\n",
        "\n",
        "Importantly, two words with similar meanings will also have similar vectors (meaning, more or less, that all of the numbers in the vectors are similar in value). So you can tell if two words are synonymous by checking the similarity between their vectors.\n",
        "\n",
        "But what about the meaning of *entire sentences*? This is a little bit more difficult, and there are a number of different and sophisticated solutions (including Google's [Universal Sentence Encoder](https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/2) and [doc2vec](https://radimrehurek.com/gensim/models/doc2vec.html)). It turns out, though, that you can get a pretty good vector for a sentence simply by *averaging together the vectors for the words in the sentence*. We'll call such vectors *sentence vectors* or *summary vectors*.\n",
        "\n",
        "Intuitively, this makes sense: finding the average is a time-tested method in statistics of characterizing a data set. It's apparently no different with word vectors. This method has the additional benefits of being fast and easy to explain.\n",
        "\n",
        "## Writing the code\n",
        "\n",
        "With your understanding of these concepts, we can actually start writing some code. For our semantic similarity chatbot, we need:\n",
        "\n",
        "* Pre-trained word vectors\n",
        "* A corpus of conversations\n",
        "* Some code to parse conversations into turns and map each turn to its response\n",
        "* Some code that can average the word vectors in some text to produce a sentence vector\n",
        "* A database that will allow us to store sentence vectors and look them up by similarity\n",
        "* Some code to take an incoming conversational turn, turn it into a sentence vector, and then look up the most similar vector in the database\n",
        "\n",
        "Let's take these one-by-one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gQ6PaStQ171",
        "colab_type": "text"
      },
      "source": [
        "### Pre-trained word vectors\n",
        "\n",
        "We're going to use [spaCy](https://spacy.io), a wonderful Python library for natural language processing, both to tokenize text (i.e., turn text into a list of words) and for its database of word vectors. To install spaCy, run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYaOlN9zRAn4",
        "colab_type": "code",
        "outputId": "9a477106-061d-4a0d-8408-7d212d24ba3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.1.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q_HHp3xRNtA",
        "colab_type": "text"
      },
      "source": [
        "It turns out that spaCy requires a \"model\" file, which is a bundle of statistical information that allows the library to parse text into words and parts of speech. While spaCy comes with a model when you install it, that model does *not* include word vectors, so you'll need to download a model that does include them. For English, I recommend `en_core_web_lg`, which you can download and install by running the cell below. (The model file is fairly large and might take a while to download.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDFEXEfhRMCB",
        "colab_type": "code",
        "outputId": "1df96f19-d87d-4758-f947-199bb9767687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz#egg=en_core_web_lg==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I9yIqLWSSA9",
        "colab_type": "text"
      },
      "source": [
        "**Si** te da ERROR, ves a Entorno de ejecucion > Reiniciar entorno de ejecucion --> Click, o lo que es lo mismo, pulsa Ctrl + M. The code in the following cell loads `spacy` and the model you just downloaded:![texto alternativo](https://drive.google.com/open?id=1EhrNsjwzA6uQeRFCEd9JY5xSkMM-SFGL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RY5ytQYSKZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMYW6B-KSXK3",
        "colab_type": "text"
      },
      "source": [
        "You can look up the word vector for a particular word using spaCy right out the box like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J9-D9EESjbE",
        "colab_type": "code",
        "outputId": "8a1bed0a-a123-4d12-d1db-d236b44eee01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nlp.vocab['juan'].vector # replace cheese with whatever word you want!"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.5432e-01, -6.5490e-02,  2.2476e-02, -3.2595e-01,  2.2633e-01,\n",
              "       -2.7835e-01, -2.5954e-01,  1.0582e-02,  2.5703e-02,  8.9459e-02,\n",
              "       -5.4213e-01, -3.9380e-01,  1.3977e-01, -9.1541e-03, -8.0629e-02,\n",
              "       -1.3782e-01, -3.8971e-01,  4.6961e-01, -6.6352e-02,  2.9489e-01,\n",
              "        3.2856e-01,  8.2969e-02,  5.2029e-01, -2.8582e-01, -4.9634e-02,\n",
              "       -2.2899e-01, -7.9279e-02,  2.6567e-01,  2.9916e-01,  2.7403e-01,\n",
              "        1.4636e-01, -2.8759e-01,  2.0046e-01, -5.4589e-01, -1.0327e+00,\n",
              "       -2.9084e-03, -2.9957e-01, -3.6627e-01, -2.8308e-01,  6.5666e-02,\n",
              "        4.6541e-01,  2.1714e-01,  5.1466e-01,  2.0727e-02,  3.4570e-01,\n",
              "        2.5616e-01,  5.4556e-01, -2.1375e-01,  7.6850e-01,  5.6746e-01,\n",
              "       -5.6860e-01, -1.2662e-01,  5.1034e-02, -1.2119e-01,  3.1355e-01,\n",
              "        2.7982e-02,  5.8021e-01,  6.1362e-01, -2.2726e-01, -2.3127e-01,\n",
              "        1.2264e-01, -3.6823e-01, -2.3497e-01,  8.7014e-03,  5.2562e-01,\n",
              "       -1.3970e-01, -8.3476e-02,  3.1328e-01, -5.3219e-01, -6.8344e-01,\n",
              "       -3.6635e-01, -2.4679e-02, -8.7927e-02,  2.0344e-01,  3.7653e-01,\n",
              "       -3.1692e-01,  8.5644e-02,  2.0715e-01,  1.3725e-01,  2.2219e-01,\n",
              "        2.0568e-01,  3.9866e-01,  8.5161e-02, -2.7637e-01, -2.6259e-01,\n",
              "        3.4285e-01,  1.1235e+00, -4.2867e-01, -6.1321e-01, -2.8451e-01,\n",
              "       -1.1355e-01, -5.3404e-01, -1.2630e-01, -5.8497e-01,  3.9203e-01,\n",
              "        6.5429e-02,  1.1691e-03, -5.6566e-01,  7.7947e-02,  2.1960e-01,\n",
              "       -1.9034e-01, -8.5134e-02, -7.1135e-02,  6.3027e-01,  1.8355e-01,\n",
              "       -9.7860e-01, -2.7433e-01, -4.8567e-01, -2.7629e-01,  2.6607e-02,\n",
              "        2.5016e-01, -4.7519e-01,  4.9994e-01, -2.8513e-01,  2.8136e-01,\n",
              "        1.6007e-01,  5.8255e-01, -3.8310e-01, -1.3369e-01,  1.3928e-01,\n",
              "       -6.6928e-01,  5.9875e-02, -1.5653e-01,  1.1694e-01, -1.3818e-02,\n",
              "        9.6414e-02, -3.7294e-01,  1.1730e-01,  5.6544e-02, -1.6756e-02,\n",
              "       -6.6974e-01, -8.4951e-02,  1.6889e-01, -2.3318e-01,  6.3699e-02,\n",
              "        1.3545e-01, -2.4600e-01,  1.8994e-01,  1.2374e-01, -7.5289e-02,\n",
              "       -1.2372e+00,  2.0316e-01, -3.4860e-01,  3.2619e-01, -1.0283e-01,\n",
              "        1.7455e-01,  6.8516e-01, -1.5170e-02, -1.7030e-01, -7.0634e-01,\n",
              "        4.8647e-01,  1.5928e-01, -4.7069e-02, -1.0169e-01,  6.9776e-02,\n",
              "        2.1058e-01, -2.0614e-01, -2.0059e-01,  3.1173e-01,  4.0326e-01,\n",
              "        8.1106e-01, -3.4737e-01, -3.7962e-01,  6.4181e-01,  6.0118e-01,\n",
              "        8.4510e-01,  2.4697e-01,  6.6566e-01,  2.1435e-01,  1.8632e-01,\n",
              "       -3.6885e-01,  1.3187e-01, -4.3646e-01, -2.6762e-02,  6.1979e-03,\n",
              "       -1.7278e-01,  3.0281e-01,  1.4315e-01, -3.3083e-01,  4.9977e-01,\n",
              "       -2.5749e-01, -1.4921e-01, -2.8351e-01,  6.2336e-02,  8.1736e-01,\n",
              "        2.5369e-01, -1.4320e-01,  5.3065e-01,  9.1146e-02,  1.4536e-01,\n",
              "        2.1279e-01, -1.0620e-01,  1.8901e-01, -3.8336e-01, -4.7291e-01,\n",
              "        1.4795e-01,  1.7848e-01, -2.2270e-01,  1.4078e-01,  8.1804e-01,\n",
              "        5.7599e-02, -2.6169e-01,  8.0467e-02,  8.4596e-02, -2.5127e-01,\n",
              "       -4.5728e-01, -1.5717e-01,  1.9329e-01, -4.2874e-01,  4.4350e-01,\n",
              "       -1.3906e-01,  7.7042e-01, -9.1961e-02,  1.3991e-01, -2.9610e-01,\n",
              "        4.4744e-02,  4.9730e-01,  4.7660e-01,  8.9640e-02,  2.3347e-01,\n",
              "        1.1681e-01, -4.1925e-02, -1.1044e-01,  4.5811e-01,  2.3754e-02,\n",
              "       -2.0215e-01, -5.7065e-01,  4.9081e-01,  1.5448e-01,  1.7240e-01,\n",
              "       -6.5509e-02,  3.8580e-01,  5.2485e-02,  8.9928e-02, -6.9108e-01,\n",
              "        1.6000e-01, -2.2465e-01, -3.7245e-01, -9.1125e-02,  2.0106e-01,\n",
              "        3.7633e-02, -3.6748e-01,  1.7756e-01, -9.8923e-02, -1.7127e-01,\n",
              "       -4.0332e-01,  6.1014e-01,  1.7108e-01, -6.9117e-03, -3.4390e-01,\n",
              "        4.5279e-01, -4.9039e-02,  9.0081e-02,  8.0091e-01,  5.7854e-01,\n",
              "        3.4250e-01, -8.1837e-01, -6.8616e-01,  3.0565e-01, -3.0536e-01,\n",
              "       -4.0126e-02, -1.1453e-01, -6.4926e-01,  6.1087e-02, -2.3940e-01,\n",
              "       -1.1065e+00,  2.6487e-01, -1.7723e-01, -8.0279e-01,  2.0645e-01,\n",
              "       -7.9768e-01,  8.8680e-01, -1.7547e-01,  7.1317e-01,  5.8953e-01,\n",
              "       -3.4802e-02,  3.6161e-01, -2.6379e-01,  9.1144e-02,  2.7417e-01,\n",
              "       -1.2848e-01, -2.7434e-01, -1.8078e-02,  2.6047e-01,  6.6695e-01,\n",
              "        4.8919e-01,  2.6704e-01, -4.6154e-01, -4.8699e-01, -1.9014e-02,\n",
              "        8.0219e-01,  1.1237e-01, -4.9448e-01,  1.1676e-01,  8.9343e-01,\n",
              "        3.5546e-02,  4.0504e-01, -6.2081e-01, -1.0317e-01, -3.6345e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z06aIT2uGxe5",
        "colab_type": "text"
      },
      "source": [
        "It might not look much, but that list of three hundred numbers is spaCy's idea of what \"cheese\" means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q65ocsUUSsJV",
        "colab_type": "text"
      },
      "source": [
        "### Parsing a corpus of conversations\n",
        "\n",
        "So now we need some data for the bot. In particular, we need some conversations: the text of the turns along with information about which turn is in response to which. Fortunately, some researchers at Cornell University have made available a very interesting corpus of conversations: [The Cornell Movie Dialog Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), containing \"220,579 conversational exchanges between 10,292 pairs of movie characters.\" Very cool. The data is stored in several plain text files, which you can download by running the following cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4DzyPM9AFjK",
        "colab_type": "code",
        "outputId": "0b24cb80-86b8-4cb7-b0f2-4202a755c2f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "!curl -L -O http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 9684k  100 9684k    0     0  2601k      0  0:00:03  0:00:03 --:--:-- 2601k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnAPBxSnAV89",
        "colab_type": "code",
        "outputId": "ba38d103-4b8e-4ab0-8fcf-4e37dc7c119a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "!unzip cornell_movie_dialogs_corpus.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cornell_movie_dialogs_corpus.zip\n",
            "replace cornell movie-dialogs corpus/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dAMYW22GNAJ",
        "colab_type": "text"
      },
      "source": [
        "We'll be working with two files from this corpus. One file (`movie_lines.txt`) has the movie lines themselves, associated with a short unique identifier; another file (`movie_conversations.txt`) has lists of which lines occurred together in conversations, in the order in which they occurred. The following two cells parse these two files and create lookup dictionaries that associate unique IDs to lines (`movie_lines`) and each line to the line that follows it (`responses`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHr4xBS_AfBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movie_lines = {}\n",
        "for line in open(\"./cornell movie-dialogs corpus/movie_lines.txt\",\n",
        "                 encoding=\"latin1\"):\n",
        "    line = line.strip()\n",
        "    parts = line.split(\" +++$+++ \")\n",
        "    if len(parts) == 5:\n",
        "        movie_lines[parts[0]] = parts[4]\n",
        "    else:\n",
        "        movie_lines[parts[0]] = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arbpjBT5Aj7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "responses = {}\n",
        "for line in open(\"./cornell movie-dialogs corpus/movie_conversations.txt\",\n",
        "                 encoding=\"latin1\"):\n",
        "    line = line.strip()\n",
        "    parts = line.split(\" +++$+++ \")\n",
        "    line_ids = json.loads(parts[3].replace(\"'\", '\"'))\n",
        "    for first, second in zip(line_ids[:-1], line_ids[1:]):\n",
        "        responses[first] = second"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdjf5_NTG6_O",
        "colab_type": "text"
      },
      "source": [
        "Just to make sure everything works, the cell below prints out five random pairs of conversational turns from the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWJd0N2oAsAu",
        "colab_type": "code",
        "outputId": "d143e278-b7e1-4dd7-8959-53700e078cda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "import random\n",
        "for pair in random.sample(responses.items(), 5):\n",
        "    print(\"A:\", movie_lines[pair[0]])\n",
        "    print(\"B:\", movie_lines[pair[1]])\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A: What?\n",
            "B: The wheel.\n",
            "\n",
            "A: Please, don't mess with me right now...\n",
            "B: We're not messing with you...\n",
            "\n",
            "A: You are now required to sit here with me.  Come on...be smart for a second -- what do you think will happen to us?\n",
            "B: Okay, that's very easy.  Five, six years from now I'll be in town to collect an award representing the surge in foreign coverage by local stations.\n",
            "\n",
            "A: As I suspected, you're a rank sentimentalist.\n",
            "B: Yeah? Why?\n",
            "\n",
            "A: Osgood - I can't get married in your mother's dress.  She and I - we' not built the same way.\n",
            "B: We can have it altered.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl38pCn4HlL_",
        "colab_type": "text"
      },
      "source": [
        "### Making a sentence vector\n",
        "\n",
        "To make the sentence vector for each line of dialog, we're going to use spaCy. The function `sentence_mean` below takes the spaCy object that we loaded earlier (`nlp`) and uses it to tokenize the string that you pass into the function (i.e., break it up into words). It then uses numpy's `mean()` function to find the average of the vectors, producing a new vector. The shape of the resulting vector (i.e., the number of dimensions) should be the same as the shape of the individual word vectors.\n",
        "\n",
        "(Note: I disabled the `tagger` and `parser` parts of spaCy's pipeline to improve performance. We're not using part of speech tags or dependency relations in this chatbot, so there's no reason to spend time calculating them.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JizJee4YBAdJ",
        "colab_type": "code",
        "outputId": "2be34c60-7329-42df-d4c4-a0e60feed7d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "def sentence_mean(nlp, s):\n",
        "    if s == \"\":\n",
        "        s = \" \"\n",
        "    doc = nlp(s, disable=['tagger', 'parser'])\n",
        "    return np.mean(np.array([w.vector for w in doc]), axis=0)\n",
        "sentence_mean(nlp, \"This... is a test.\").shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN9qQhQvJG-L",
        "colab_type": "text"
      },
      "source": [
        "### Similarity lookups\n",
        "\n",
        "Now that we have conversational turns and a way to vectorize those turns, we can make our database for semantic similarity lookup! The kind of \"database\" we'll need to use for this is an [approximate nearest neighbors](https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods) lookup, which allows you to store items along with the vector that represents them, and then do fast searches to find items with similar vectors (even items that weren't in the original dataset).\n",
        "\n",
        "[I made a Python library to make it easy to build databases like this](https://pypi.org/project/simpleneighbors/) called Simple Neighbors. It's a lightweight wrapper around the industrial-strength approximate nearest neighbors lookup library called [Annoy](https://pypi.python.org/pypi/annoy). To install Simple Neighbors, run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppulERZ5Tz5a",
        "colab_type": "code",
        "outputId": "69cd22da-05da-41f6-cc57-0a25e9914ba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "!pip install simpleneighbors"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: simpleneighbors in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: annoy in /usr/local/lib/python3.6/dist-packages (from simpleneighbors) (1.16.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_sFq49LKISH",
        "colab_type": "text"
      },
      "source": [
        "The cell below makes a new Simple Neighbors object called `nns` and initializes it with 300 dimensions (the shape of the word vectors in spaCy, and also the shape of our summary vectors). It then samples ten thousand random conversational turns from the Cornell corpus, finds sentence vectors for each of them, and adds them to the database. (The `np.any()` line just checks to make sure that we don't add any vectors that are all zeroes by accident—this can mess up the nearest-neighbor search.)\n",
        "\n",
        "Notes on the code below:\n",
        "\n",
        "* I decided to just sample ten thousand turns so that the index will build faster. You can change this number to your liking!\n",
        "* It only adds *turns that have responses* to the database (i.e., keys in the `responses` lookup). Because of the way the bot works, we don't need to keep track of the last turn of a conversation, since it (by definition) will have no replies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8jODdF-BHxR",
        "colab_type": "code",
        "outputId": "40681b8b-2313-4fce-e2cd-b2208dc8e51c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "from simpleneighbors import SimpleNeighbors\n",
        "\n",
        "nns = SimpleNeighbors(300)\n",
        "for i, line_id in enumerate(random.sample(list(responses.keys()), 10000)):\n",
        "    # show progress\n",
        "    if i % 1000 == 0: print(i, line_id, movie_lines[line_id])\n",
        "    line_text = movie_lines[line_id]\n",
        "    summary_vector = sentence_mean(nlp, line_text)\n",
        "    if np.any(summary_vector):\n",
        "        nns.add_one(line_id, summary_vector)\n",
        "nns.build()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 L583856 Well... [beat] let me know how your search turns out.\n",
            "1000 L433324 Can't.  I gotta drive out to Glades, then I'm meeting Ray Nicolet at ten.\n",
            "2000 L451704 Once we're out of sight, take him back to Florin and throw him in the Pit of Despair.\n",
            "3000 L168477 Hello?\n",
            "4000 L521110 You can't trust those guys.\n",
            "5000 L137585 Did I hear somethin' break? Outside left!\n",
            "6000 L278898 Honey -- I can't find those large- size Hefty trash bags!\n",
            "7000 L99430 You and me -- making love.\n",
            "8000 L117383 I don't know if I'd go that far.\n",
            "9000 L160853 No.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UOeTbDtL8l3",
        "colab_type": "text"
      },
      "source": [
        "Let's take it for a spin! The code in the following cell finds the turn most similar to the string in the variable `sentence`. (You can change this string to whatever you want.) It then uses the Simple Neighbors object to find the turn in the database with the most similar vector, and then uses the `responses` lookup to find the *response* to that turn. That response will be our bot's output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l656oBJoBLaa",
        "colab_type": "code",
        "outputId": "ae4a5fb9-e3a6-4478-a6fd-230fcd197836",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "sentence = \"I want to run.\"\n",
        "picked = nns.nearest(sentence_mean(nlp, sentence), 5)[0]\n",
        "response_line_id = responses[picked]\n",
        "\n",
        "print(\"Your line:\\n\\t\", sentence)\n",
        "print(\"Most similar turn:\\n\\t\", movie_lines[picked])\n",
        "print(\"Response to most similar turn:\\n\\t\", movie_lines[response_line_id])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your line:\n",
            "\t I want to run.\n",
            "Most similar turn:\n",
            "\t I need to go.\n",
            "Response to most similar turn:\n",
            "\t Do you know why you're here, James.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2AUaRgVPQco",
        "colab_type": "text"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "The code above is all you need to make a conversational chatbot based on semantic similarity. But there's a lot of stuff to keep track of! So I wrote a little bit of \"glue code\" to make it even easier. You can [see the source code on GitHub](https://github.com/aparrish/semanticsimilaritychatbot/); all the important stuff is [in this file](https://github.com/aparrish/semanticsimilaritychatbot/blob/master/semanticsimilaritychatbot/__init__.py). I'm going to use this library to rewrite the code above in just a few lines, and then we'll use the resulting object to make a chatbot you can use in the browser.\n",
        "\n",
        "First, download and install the library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI4sCHjmQFfu",
        "colab_type": "code",
        "outputId": "9a07c99f-c1a4-497e-f84d-fffccdcb59da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!pip install https://github.com/aparrish/semanticsimilaritychatbot/archive/master.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/aparrish/semanticsimilaritychatbot/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/aparrish/semanticsimilaritychatbot/archive/master.zip\n",
            "\u001b[K     - 10kB 22.6MB/s\n",
            "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): semanticsimilaritychatbot==0.0.1 from https://github.com/aparrish/semanticsimilaritychatbot/archive/master.zip in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: simpleneighbors in /usr/local/lib/python3.6/dist-packages (from semanticsimilaritychatbot==0.0.1) (0.0.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from semanticsimilaritychatbot==0.0.1) (2.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from semanticsimilaritychatbot==0.0.1) (1.17.3)\n",
            "Requirement already satisfied: annoy in /usr/local/lib/python3.6/dist-packages (from simpleneighbors->semanticsimilaritychatbot==0.0.1) (1.16.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->semanticsimilaritychatbot==0.0.1) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy->semanticsimilaritychatbot==0.0.1) (0.2.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy->semanticsimilaritychatbot==0.0.1) (7.0.8)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->semanticsimilaritychatbot==0.0.1) (0.9.6)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy->semanticsimilaritychatbot==0.0.1) (0.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->semanticsimilaritychatbot==0.0.1) (2.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy->semanticsimilaritychatbot==0.0.1) (0.1.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->semanticsimilaritychatbot==0.0.1) (2.21.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy->semanticsimilaritychatbot==0.0.1) (2.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy->semanticsimilaritychatbot==0.0.1) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->semanticsimilaritychatbot==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->semanticsimilaritychatbot==0.0.1) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->semanticsimilaritychatbot==0.0.1) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->semanticsimilaritychatbot==0.0.1) (3.0.4)\n",
            "Building wheels for collected packages: semanticsimilaritychatbot\n",
            "  Building wheel for semanticsimilaritychatbot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for semanticsimilaritychatbot: filename=semanticsimilaritychatbot-0.0.1-cp36-none-any.whl size=4845 sha256=93a518028f10493c0909b2519a63d7f0019b459b6c175ba1d638161719aad096\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wfrxvqyq/wheels/f7/af/8e/8a8fbef31bfbfc3b935425efa03db03825795d85f4e23f8255\n",
            "Successfully built semanticsimilaritychatbot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPGClLIPQYBw",
        "colab_type": "text"
      },
      "source": [
        "Then create a chatbot object, passing in the spaCy language object (`nlp`) and the number of dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWbiYvA-K3xv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from semanticsimilaritychatbot import SemanticSimilarityChatbot\n",
        "chatbot = SemanticSimilarityChatbot(nlp, 300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvsLgSCKQfIF",
        "colab_type": "text"
      },
      "source": [
        "The `.add_pair()` method in the object takes two strings: a turn and the response to that turn. We'll get these from the `responses` and `movie_lines` lookups, again sampling ten thousand pairs at random. This cell will take a little while to run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaEYCz70KyPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_n = 10000\n",
        "for first_id, second_id in random.sample(list(responses.items()), sample_n):\n",
        "    chatbot.add_pair(movie_lines[first_id], movie_lines[second_id])\n",
        "chatbot.build()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FixHpUnoRLdC",
        "colab_type": "text"
      },
      "source": [
        "Once you've built the database, the `.response_for()` method returns a plausible response from the database, based on semantic similarity. Try it out by changing the text between the quotation marks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-sTY8OUK1ju",
        "colab_type": "code",
        "outputId": "2b9715dd-7749-4d1f-bdb6-02653409bd15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(chatbot.response_for(\"bye baby\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Goodbye, darling.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdEXLwEHMKAh",
        "colab_type": "text"
      },
      "source": [
        "To add variety, the `.response_for()` method actually selects randomly among several similar turns. You can change the number of turns it chooses from by passing a second parameter (a number) to the method. In general, the higher the number, the greater the chance is that you'll get an unusual result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmDFQy-2MiCr",
        "colab_type": "code",
        "outputId": "71077485-18d6-469a-a891-cef5284875f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "my_turn = \"good bye baby, see you tomorrow\"\n",
        "for i in range(5, 51, 5):\n",
        "    print(\"picking from\", i, \"possible responses:\")\n",
        "    print(chatbot.response_for(my_turn, i))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "picking from 5 possible responses:\n",
            "Yeah.  I am.\n",
            "\n",
            "picking from 10 possible responses:\n",
            "Yeah.  I am.\n",
            "\n",
            "picking from 15 possible responses:\n",
            "I get scared really easy, okay.\n",
            "\n",
            "picking from 20 possible responses:\n",
            "Got a handkerchief?\n",
            "\n",
            "picking from 25 possible responses:\n",
            "Of course.\n",
            "\n",
            "picking from 30 possible responses:\n",
            "How do you do?\n",
            "\n",
            "picking from 35 possible responses:\n",
            "Maybe we could have lunch one day next week? You know, I'm downtown near Wall Street quite often.\n",
            "\n",
            "picking from 40 possible responses:\n",
            "So whatta you... You're not gonna come back to New York?\n",
            "\n",
            "picking from 45 possible responses:\n",
            "No, Dad.  I don't want to...I mean, I just don't want...\n",
            "\n",
            "picking from 50 possible responses:\n",
            "Eight.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZw0DgPiRgz4",
        "colab_type": "text"
      },
      "source": [
        "The Semantic Similarity Chatbot object has a `.save()` method that saves the pre-built database to disk, using a filename prefix you supply. (It saves three different files: `<prefix>.annoy`, `<prefix>-data.pkl`, and `<prefix>-chatbot.pkl`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPgATDpnLTYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chatbot.save(\"movielines-10k-sample\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVM_GcjoR9pG",
        "colab_type": "text"
      },
      "source": [
        "You can use a previously-saved database using the `.load()` class method, like so. (This means you don't have to build the database again: you can just load it and start calling `.response_for()`.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wutboh4MLkja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chatbot = SemanticSimilarityChatbot.load(\"movielines-10k-sample\", nlp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6IOQ9DPNrOR",
        "colab_type": "code",
        "outputId": "b4b07203-5529-4a55-9648-f7860ae25e86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(chatbot.response_for(\"bye baby, see you tomorrow\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I'm talking to you Red and I'm telling you no. Get back to your position.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dp0gmuzSkG_",
        "colab_type": "text"
      },
      "source": [
        "If you're using this notebook on Google Colab, the following cell will download all of the files from the pre-built bot to your computer so you can use them later. (Note that you'll still have to download and install spaCy for the chatbot to work.) If you're running the notebook locally with Jupyter Notebook, the files will end up in the same directory as the notebook file itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCeo-cHdSUxg",
        "colab_type": "code",
        "outputId": "824ae2e5-9458-4324-e010-647c8944c80d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.download('movielines-10k-sample.annoy')\n",
        "files.download('movielines-10k-sample-data.pkl')\n",
        "files.download('movielines-10k-sample-chatbot.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-8bb23724ad8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movielines-10k-sample.annoy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movielines-10k-sample-data.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movielines-10k-sample-chatbot.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1wElvooTBjp",
        "colab_type": "text"
      },
      "source": [
        "## Making it interactive\n",
        "\n",
        "If you're using this notebook in Google Colab, the following cell will create a little interactive interface for chatting with the bot that you just built. Run the two cells below and start typing into the box."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSjtkXigBuRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chatbot_html = \"\"\"\n",
        "<style type=\"text/css\">#log p { margin: 5px; font-family: sans-serif; }</style>\n",
        "<div id=\"log\"\n",
        "     style=\"box-sizing: border-box;\n",
        "            width: 600px;\n",
        "            height: 32em;\n",
        "            border: 1px grey solid;\n",
        "            padding: 2px;\n",
        "            overflow: scroll;\">\n",
        "</div>\n",
        "<input type=\"text\" id=\"typehere\" placeholder=\"type here!\"\n",
        "       style=\"box-sizing: border-box;\n",
        "              width: 600px;\n",
        "              margin-top: 5px;\">\n",
        "<script>\n",
        "function paraWithText(t) {\n",
        "    let tn = document.createTextNode(t);\n",
        "    let ptag = document.createElement('p');\n",
        "    ptag.appendChild(tn);\n",
        "    return ptag;\n",
        "}\n",
        "document.querySelector('#typehere').onchange = async function() {\n",
        "    let inputField = document.querySelector('#typehere');\n",
        "    let val = inputField.value;\n",
        "    inputField.value = \"\";\n",
        "    let resp = await getResp(val);\n",
        "    let objDiv = document.getElementById(\"log\");\n",
        "    objDiv.appendChild(paraWithText('😀: ' + val));\n",
        "    objDiv.appendChild(paraWithText('🤖: ' + resp));\n",
        "    objDiv.scrollTop = objDiv.scrollHeight;\n",
        "};\n",
        "async function colabGetResp(val) {\n",
        "    let resp = await google.colab.kernel.invokeFunction(\n",
        "        'notebook.get_response', [val], {});\n",
        "    return resp.data['application/json']['result'];\n",
        "}\n",
        "async function webGetResp(val) {\n",
        "    let resp = await fetch(\"/response.json?sentence=\" + \n",
        "        encodeURIComponent(val));\n",
        "    let data = await resp.json();\n",
        "    return data['result'];\n",
        "}\n",
        "</script>\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHilVz_Yy3Th",
        "colab_type": "code",
        "outputId": "0dbb7793-cedf-4028-e393-dd824bbf66f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.HTML(chatbot_html + \\\n",
        "                             \"<script>let getResp = colabGetResp;</script>\"))\n",
        "\n",
        "def get_response(val):\n",
        "    resp = chatbot.response_for(val)\n",
        "    return IPython.display.JSON({'result': resp})\n",
        "\n",
        "output.register_callback('notebook.get_response', get_response)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<style type=\"text/css\">#log p { margin: 5px; font-family: sans-serif; }</style>\n",
              "<div id=\"log\"\n",
              "     style=\"box-sizing: border-box;\n",
              "            width: 600px;\n",
              "            height: 32em;\n",
              "            border: 1px grey solid;\n",
              "            padding: 2px;\n",
              "            overflow: scroll;\">\n",
              "</div>\n",
              "<input type=\"text\" id=\"typehere\" placeholder=\"type here!\"\n",
              "       style=\"box-sizing: border-box;\n",
              "              width: 600px;\n",
              "              margin-top: 5px;\">\n",
              "<script>\n",
              "function paraWithText(t) {\n",
              "    let tn = document.createTextNode(t);\n",
              "    let ptag = document.createElement('p');\n",
              "    ptag.appendChild(tn);\n",
              "    return ptag;\n",
              "}\n",
              "document.querySelector('#typehere').onchange = async function() {\n",
              "    let inputField = document.querySelector('#typehere');\n",
              "    let val = inputField.value;\n",
              "    inputField.value = \"\";\n",
              "    let resp = await getResp(val);\n",
              "    let objDiv = document.getElementById(\"log\");\n",
              "    objDiv.appendChild(paraWithText('😀: ' + val));\n",
              "    objDiv.appendChild(paraWithText('🤖: ' + resp));\n",
              "    objDiv.scrollTop = objDiv.scrollHeight;\n",
              "};\n",
              "async function colabGetResp(val) {\n",
              "    let resp = await google.colab.kernel.invokeFunction(\n",
              "        'notebook.get_response', [val], {});\n",
              "    return resp.data['application/json']['result'];\n",
              "}\n",
              "async function webGetResp(val) {\n",
              "    let resp = await fetch(\"/response.json?sentence=\" + \n",
              "        encodeURIComponent(val));\n",
              "    let data = await resp.json();\n",
              "    return data['result'];\n",
              "}\n",
              "</script>\n",
              "<script>let getResp = colabGetResp;</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1CJg2lG67KB",
        "colab_type": "text"
      },
      "source": [
        "If you're not using Colab, try the following two cells to install [Flask](http://flask.pocoo.org) and run a little web server from your notebook that lets you chat with the bot. Click on the link that appears below the second cell to open up the chat in a new window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4hcd0wU4jGK",
        "colab_type": "code",
        "outputId": "499f750c-7cd6-4496-a191-db60b0016672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "!pip install flask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/e7/08578774ed4536d3242b14dacb4696386634607af824ea997202cd0edb4b/Flask-1.0.2-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 4.1MB/s \n",
            "\u001b[?25hCollecting click>=5.1 (from flask)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl (71kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask) (0.14.1)\n",
            "Collecting itsdangerous>=0.24 (from flask)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/b4/a60bcdba945c00f6d608d8975131ab3f25b22f2bcfe1dab221165194b2d4/itsdangerous-0.24.tar.gz (46kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from flask) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->flask) (1.0)\n",
            "Building wheels for collected packages: itsdangerous\n",
            "  Running setup.py bdist_wheel for itsdangerous ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/2c/4a/61/5599631c1554768c6290b08c02c72d7317910374ca602ff1e5\n",
            "Successfully built itsdangerous\n",
            "Installing collected packages: click, itsdangerous, flask\n",
            "Successfully installed click-6.7 flask-1.0.2 itsdangerous-0.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25bjOkzX4dC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flask import Flask, request, jsonify\n",
        "app = Flask(__name__)\n",
        "@app.route(\"/response.json\")\n",
        "def response():\n",
        "    sentence = request.args['sentence']\n",
        "    return jsonify(\n",
        "        {'result': chatbot.response_for(sentence)})\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return chatbot_html + \"<script>let getResp = webGetResp;</script>\"\n",
        "app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH-If5m07h8_",
        "colab_type": "text"
      },
      "source": [
        "## Some things to try\n",
        "\n",
        "If you enjoyed following along, here are some things to try:\n",
        "\n",
        "* Use the metadata file that comes with the Cornell corpus to make a chatbot that only uses lines from a particular genre of movie. (How is a comedy chatbot different from an action chatbot?)\n",
        "* Use a different corpus of conversation altogether. Your own chat logs? Conversational exchanges from a novel? Transcripts of interviews on news programs?\n",
        "* Incorporate some context from the conversation when vectorizing the turns. (You might, for example, include the average of not just the given turn but also the turn that preceded it.)"
      ]
    }
  ]
}